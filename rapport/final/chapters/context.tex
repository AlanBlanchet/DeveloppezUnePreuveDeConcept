\section{Contexte}

% Old algorithms



% Currently

% Proposed

Afin de comparer les résultats obtenus par NGU, des environnements standards sont utilisés comme ceux d'OpenAI Gym \cite{OpenAI_Gym}. Ces environnements représentent des jeux vidéo utilisés pour tester les performances des algorithmes de RL. Ils sont simples à utiliser mais cependant, ces environnements ne sont pas adaptés à des problèmes réels comme par exemple pour la robotique car ils ne prennent pas en compte les différents facteurs des environnements.

NGU essaye de résoudre les problèmes d'exploration lié à une policy (politique en français) $\pi$. La policy est une fonction qui prend en entrer une observation de l'environnement $s$ et retourne une action $a$ à effectuer. L'objectif de NGU est de trouver cette policy $\pi$ qui maximise la somme des récompenses futures tout en explorant un maximum son environnement. La récompense future est définie comme la somme des récompenses à partir de l'état $s$ jusqu'à la fin de l'épisode. C'est la fonction dont l'intérêt est d'être optimisée.

~\\
Pour y parvenir, il existe plusieurs familles d'algorithmes qui utilisent des approches différentes.

\subsection{Model-based}

Les algorithmes de type \textit{model-based} utilisent un modèle de l'environnement pour effectuer des \textit{simulations}. Ceux-ci incluent des algorithmes puissants comme \ucite{AlphaZero}. AlphaGo qui a réussi à battre un champion du monde de Go en 2016 est un exemple d'algorithmes de type \textit{model-based}. Cependant, ces algorithmes sont très gourmands en ressources.

\subsection{Model-free}

Les algorithmes de type \textit{model-free} n'utilisent pas de modèle de l'environnement. Dans ce type de configuration, l'agent apprend avec une méthode de \textit{trial and error}. L'agent n'est pas capable d'apprendre sur ses propres \textit{simulations}. Par exemple, l'agent n'est pas capable de se dire \textit{"Et si j'avais pris cette action ? J'aurais mieux fait ?"} et doit donc apprendre directement grâce à l'environnement en suivant une ligne directrice. Pour le voir différement, dans un environnement avec 4 actions, le modèle \textit{model-free} ne pourras par conçevoir l'issue d'effectuer chaque action pour une même observation. Il doit inévitablement choisir une action et peut uniquement s'appuyer sur l'observation récupérée de l'environnement pour s'améliorer. Plusieurs algorithmes populaires sont issus de ce groupe d'algorithmes tel que \ucite{PPO} ou \ucite{TD3}. C'est le cas de NGU.

\subsection{On-policy}

Les algorithmes de type \textit{on-policy} peuvent être \textit{model-free} ou \textit{model-based}. Ceux-ci utilisent une seule policy $\pi$ pour explorer l'environnement et y effectuer des actions. Les actions que l'agent réalise dans l'environnement sont basées sur la policy $\pi_t$ qui est une fonction qui possède des poids définis par $\theta_t$. La policy $\pi_t$ est donc définie par $\pi_t(s, \theta_t)$, $t$ représentant le \textit{timestep} d'une expérience. L'agent va donc apprendre à optimiser les poids $\theta_t$ de la policy $\pi_t$ en fonction des récompenses qu'il reçoit. La différence primordiale avec une méthode \textit{off-policy} est que l'agent est limité à un apprentissage sur une \textit{transition} qui est issue de $\pi_t(s, \theta_t)$ uniquement. Ainsi, en effectuant une mise à jour de notre policy, on ne peut plus utiliser la \textit{transition} car nous sommes à l'état $s_{t+1}$ et non plus à l'état $s_t$. Cette méthode est généralement plus simple à implémenter car elle ne nécessite pas de stocker les \textit{transitions} dans une mémoire tampon. Cependant, elle est moins efficace car l'agent ne peut pas apprendre sur des \textit{transitions} passées.

\subsection{Off-policy}

Les algorithmes de type \textit{off-policy} peuvent être \textit{model-free} ou \textit{model-based} tout comme le type \textit{on-policy}. La différence fondamentale est qu'on peut tout à fait réutiliser des expériences passées pour entrainer notre agent. Cela permet d'augmenter la vitesse d'apprentissage de l'agent car il peut apprendre sur des \textit{transitions} qu'il a déjà effectuées. Ainsi, on dit qu'on gagne en \textit{sample efficiency}. Cependant, cette méthode est plus complexe à implémenter car il faut stocker les \textit{transitions} dans une mémoire tampon. De plus, il faut faire attention à ne pas utiliser des \textit{transitions} trop anciennes car celles-ci ne sont peut-être plus représentatives de l'environnement actuel. Egalement, il faut faire attention à ne pas utiliser des \textit{transitions} qui se chevauchent entre épisodes car cela pourrait biaiser l'apprentissage de l'agent. Ainsi, plein de concepts intéressants découlent de cette méthode comme les \textit{replay buffer}s ou du \textit{importance sampling} \cite{PER}.